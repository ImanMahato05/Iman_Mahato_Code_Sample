import tensorflow as tf
import numpy as np
import data_import as data_import
import os
import datetime
import math
import utils
from data_import import DataSet
from gtnnwr import S_T_NETWORK
from gtnnwr import GTW_NETWORK
from gtnnwr import DIAGNOSIS
import matplotlib as mpl

mpl.use('Agg')  # No display
from xlrd import open_workbook
from xlutils.copy import copy
import xlwt
import shutil
from init_config import ConfigSetting

if __name__ == '__main__':

    # Computer name, used to synchronize results generated by different machines.
    host_name = os.environ['COMPUTERNAME']
    data_setting_file = "setting_para/setting_data_gtnnwr_simulation_st.cfg"
    # data_setting_file = "setting_para/setting_data_gtnnwr_zj.cfg"
    model_setting_file = "setting_para/setting_model.cfg"

    # global setting
    cp = ConfigSetting(data_setting_path=data_setting_file, model_setting_path=model_setting_file)
    diff_weight, batch_norm, create_force, random_fixed, base_path, log_y, simple_stnn, stannr, seed, \
    iter_num, stop_num, stop_iter, train_r2_cri, \
    test_r2_cri, log_delete, test_model, cv_fold = cp.get_global_para()

    col_data_x, col_data_y, col_coordx, col_coordy, col_coordt, seasons, models, simple_stnns, stannrs, simple_gtwnns, \
    datafile, log_y, normalize_y, train_ratio, validation_ratio, st_weight_init, \
    gtw_weight_init, epochs, start_lr, max_lr, total_up_steps, up_decay_steps, maintain_maxlr_steps, \
    delay_steps, delay_rate, keep_prob_ratio, val_early_stopping, val_early_stopping_begin_step, \
    model_comparison_criterion = cp.get_data_para()

    snn_hidden_layer_count, snn_neural_sizes, snn_output_size, tnn_hidden_layer_count, tnn_neural_sizes, tnn_output_size, \
    stnn_hidden_layer_count, stnn_neural_sizes, stnn_output_size, gtwnn_factor, gtwnn_hidden_node_limit, \
    gtwnn_max_layer_count, kernel_size, kernel_num, smooth_coords_path, x_len, \
    cnngtwnn_factor, cnngtwnn_hidden_node_limit, cnngtwnn_max_layer_count = cp.get_model_structure()

    datafile_origin = datafile

    for model_index in range(0, len(models) * iter_num):
        if model_index > 0:
            tf.reset_default_graph()

        model = models[model_index % len(models)]
        date_str = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
        print(datetime.datetime.now().strftime('%Y/%m/%d %H:%M:%S') + ' Finding ' + str(
            stop_iter + 1) + ' excellent result.')

        if '*' in datafile_origin:
            datafile = datafile_origin.replace('*', str(model_index + 1))

        datafile_name = datafile[0:datafile.rfind('.')]
        # Simple spacetime network, no hidden layers
        simple_stnn = simple_stnns[model_index % len(models)]

        # Does it include autocorrelation networks?
        stannr = stannrs[model_index % len(models)]

        # Should the weighted network output be simplified to a beta value?
        simple_gtwnn = simple_gtwnns[model_index % len(models)]

        # model para
        data_path = base_path + datafile
        no_space, s_no_network, no_time, t_no_network, st_no_network, s_each_dir, t_cycle, s_activate_fun, t_activate_fun, st_activate_fun, \
        gtw_activate_fun, no_cnn, dataset_path = cp.get_model_para(model)

        # reading data
        train_val_sets, train_sets, val_sets, test_sets, miny, maxy, dataname, train_coords = data_import.init_dataset_cv(
            data_path,
            train_ratio=train_ratio,
            validation_ratio=validation_ratio,
            cv_fold=cv_fold,
            s_each_dir=s_each_dir,
            t_cycle=t_cycle,
            log_y=log_y,
            normalize_y=normalize_y,
            date_str=date_str,
            create_force=create_force,
            random_fixed=random_fixed,
            seed=seed,
            col_data_x=col_data_x,
            col_data_y=col_data_y,
            col_date=col_coordt,
            date_numeric=True,
            col_coordx=col_coordx,
            col_coordy=col_coordy)

        # If the y-values ​​are not normalized, no transformation is needed; set the maximum and minimum values ​​to 1 and 0, respectively.
        if not normalize_y:
            miny = 0
            maxy = 1

        outputs_train = []
        outputs_val = []
        outputs_test = []

        for cv_index in range(0, len(train_sets)):
            print(datetime.datetime.now().strftime('%Y/%m/%d %H:%M:%S') + ' Cross Validation ' + str(cv_index + 1))
            train_val_set = train_val_sets[cv_index]
            train_set = train_sets[cv_index]
            valiset = val_sets[cv_index]
            testset = test_sets[cv_index]
            train_coord = train_coords[cv_index]

            if cv_index > 0:
                date_str = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
                tf.reset_default_graph()

            # total training and validation datasets
            x_train_val = train_val_set.x_data
            y_train_val = train_val_set.y_data
            s_dis_train_val = train_val_set.space_dis
            t_dis_train_val = train_val_set.time_dis

            # Training dataset
            x_train = train_set.x_data
            y_train = train_set.y_data
            s_dis_train = train_set.space_dis
            t_dis_train = train_set.time_dis
            # Validation dataset
            x_vali = valiset.x_data
            y_vali = valiset.y_data
            s_dis_vali = valiset.space_dis
            t_dis_vali = valiset.time_dis

            # Testing dataset
            x_test = testset.x_data
            y_test = testset.y_data
            s_dis_test = testset.space_dis
            t_dis_test = testset.time_dis

            sample_size = x_train.shape[0]
            dis_size = s_dis_train.shape[1]
            # dropout的keep_prob
            keep_prob_st = tf.placeholder(tf.float32)
            keep_prob_gtw = tf.placeholder(tf.float32)
            bn_is_training = tf.placeholder(tf.bool)

            gtnnwr_output_size = train_set.x_data.shape[1]
            x_data = tf.placeholder(tf.float32, [None, gtnnwr_output_size])
            y_data = tf.placeholder(tf.float32, [None, 1])
            distance = tf.placeholder(tf.float32, [None, None])

            if not no_space:
                s_input_size = s_dis_train.shape[2]

                s_network = S_T_NETWORK('space', sample_size, dis_size, s_input_size, snn_hidden_layer_count,
                                        snn_neural_sizes,
                                        s_activate_fun, keep_prob_st, output_layer_size=snn_output_size,
                                        batch_norm=batch_norm, bn_is_training=bn_is_training,
                                        weight_init=st_weight_init, diff_weight=diff_weight, no_network=s_no_network)

                snn_output_size = s_network.output_size
                snn_output = tf.reshape(s_network.output, [-1, dis_size, snn_output_size])
                snn_x_data = s_network.dist_data
            else:
                snn_output_size = 0
                snn_output = None
                snn_x_data = tf.placeholder(tf.float32, [None, None, None])

            if not no_time:
                t_input_size = t_dis_train.shape[2]

                t_network = S_T_NETWORK('time', sample_size, dis_size, t_input_size, tnn_hidden_layer_count,
                                        tnn_neural_sizes,
                                        t_activate_fun, keep_prob_st, output_layer_size=tnn_output_size,
                                        batch_norm=batch_norm, bn_is_training=bn_is_training,
                                        weight_init=st_weight_init, diff_weight=diff_weight, no_network=t_no_network)

                tnn_output_size = t_network.output_size
                tnn_output = tf.reshape(t_network.output, [-1, dis_size, tnn_output_size])
                tnn_x_data = t_network.dist_data
            else:
                tnn_output_size = 0
                tnn_output = None
                tnn_x_data = tf.placeholder(tf.float32, [None, None, None])

            # Transform the inputs and outputs of SNN and TNN, and input them into STNN.
            if no_space:
                st_input = tnn_output
            elif no_time:
                st_input = snn_output
            else:
                st_input = tf.concat([snn_output, tnn_output], 2)
            st_input_size = snn_output_size + tnn_output_size
            st_input = tf.reshape(st_input, [-1, st_input_size])

            if simple_stnn:
                stnn_hidden_layer_count = 0
                stnn_neural_sizes = [0]
                stnn_output_size = 1

            st_network = S_T_NETWORK('space_time', sample_size, dis_size, st_input_size, stnn_hidden_layer_count,
                                     stnn_neural_sizes,
                                     st_activate_fun, keep_prob_st, output_layer_size=stnn_output_size,
                                     dist_data=st_input, batch_norm=batch_norm,
                                     bn_is_training=bn_is_training, weight_init=st_weight_init, diff_weight=diff_weight,
                                     no_network=st_no_network)

            gtwnn_hidden_layer_count, gtwnn_neural_sizes = utils.hidden_layers(dis_size, factor=gtwnn_factor,
                                                                               hidden_node_limit=gtwnn_hidden_node_limit,
                                                                               max_layer_count=gtwnn_max_layer_count)

            stnn_output_size = st_network.output_size
            gtwnn_input = tf.reshape(st_network.output, [-1, dis_size * stnn_output_size])

            gtw_network = GTW_NETWORK('gtw_network', gtwnn_input, gtwnn_hidden_layer_count, gtwnn_neural_sizes,
                                      gtnnwr_output_size,
                                      gtw_activate_fun, keep_prob_gtw, batch_norm=True, bn_is_training=bn_is_training,
                                      weight_init=gtw_weight_init)

            # Multiple linear regression (using training dataset)
            # CV is calculated using the training set here.
            linear_beta = tf.squeeze(utils.linear(tf.constant(train_set.x_data, dtype=tf.float32),
                                                  tf.constant(np.reshape(train_set.y_data, [len(train_set.y_data), 1]),
                                                              dtype=tf.float32)))
            linear_beta_mat = tf.diag(linear_beta)
            #Return part
            yhat = tf.diag_part(tf.matmul(tf.matmul(x_data, linear_beta_mat), tf.transpose(gtw_network.gtweight)))
            gtbeta = tf.transpose(tf.matmul(linear_beta_mat, tf.transpose(gtw_network.gtweight)))
            loss = tf.reduce_mean(tf.square(yhat - tf.squeeze(y_data)))

            # This section contains hypothesis testing for GTNNWR.
            diagnosis = DIAGNOSIS(x_data, y_data, yhat, linear_beta_mat, gtw_network.gtweight, miny, maxy)
            loss_convert = diagnosis.loss_convert
            loss_add_reg = diagnosis.loss_add_reg
            ave_abs_error = diagnosis.ave_abs_error
            ave_rel_error = diagnosis.ave_rel_error

            r2_pearson = diagnosis.r2_pearson
            r2_coeff = diagnosis.r2_coeff
            r2_adjusted_coeff = diagnosis.r2_adjusted_coeff

            AICc = diagnosis.AICc
            AIC = diagnosis.AIC

            f1 = diagnosis.f1
            f2 = diagnosis.f2
            f3_dict = diagnosis.f3_dict
            f3_dict_2 = diagnosis.f3_dict_2

            global_step = tf.Variable(0, trainable=False)
            # Custom learning rate, increase first then decrease
            learning_rate_decay = utils.exponential_decay_norm(start_lr, max_lr, global_step, delay_steps, delay_rate,
                                                               total_up_steps, up_decay_steps, maintain_maxlr_steps,
                                                               staircase=True)

            # Initialization of batch normalization
            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
            with tf.control_dependencies(update_ops):
                # Ensures that we execute the update_ops before performing the train_step
                train = tf.train.GradientDescentOptimizer(learning_rate_decay).minimize(loss_add_reg,
                                                                                        global_step=global_step)

            # Initialize variables!
            init = tf.global_variables_initializer()

            feed_train = {x_data: x_train, y_data: y_train, snn_x_data: s_dis_train, tnn_x_data: t_dis_train,
                          keep_prob_st: 1,
                          keep_prob_gtw: 1, bn_is_training: True}
            feed_val = {x_data: x_vali, y_data: y_vali, snn_x_data: s_dis_vali, tnn_x_data: t_dis_vali, keep_prob_st: 1,
                        keep_prob_gtw: 1, bn_is_training: False}
            feed_train_val = {x_data: x_train_val, y_data: y_train_val, snn_x_data: s_dis_train_val,
                              tnn_x_data: t_dis_train_val,
                              keep_prob_st: 1, keep_prob_gtw: 1, bn_is_training: False}
            feed_test = {x_data: x_test, y_data: y_test, snn_x_data: s_dis_test, tnn_x_data: t_dis_test,
                         keep_prob_st: 1, keep_prob_gtw: 1, bn_is_training: False}

            # Store variables for early stopping
            #This variable must be placed outside, not inside, otherwise restore seems to cause problems.
            saver = tf.train.Saver()
            # summary
            log_dir = 'Data/logs/' + datafile_name + '/' + model

            writer_train = tf.summary.FileWriter(log_dir + '/' + date_str + '/train/', tf.get_default_graph())
            writer_validation = tf.summary.FileWriter(log_dir + '/' + date_str + '/validation/', tf.get_default_graph())
            writer_test = tf.summary.FileWriter(log_dir + '/' + date_str + '/test/', tf.get_default_graph())

            summary_loss = tf.summary.scalar('loss', loss)
            summary_pearson_r2 = tf.summary.scalar('r2_pearson', r2_pearson)
            summary_coeff_r2 = tf.summary.scalar('r2_coeff', r2_coeff)
            summary_AICc = tf.summary.scalar('AICc', AICc)
            summary_ave_abs_error = tf.summary.scalar('ave_abs_error', ave_abs_error)
            summary_ave_rel_error = tf.summary.scalar('ave_rel_error', ave_rel_error)
            summary_merged = tf.summary.merge_all()

            save_model_dir = 'Data/model_para/' + datafile_name + '/' + model + '/' + date_str + '/'
            if os.path.exists(save_model_dir):
                shutil.rmtree(save_model_dir)
            os.makedirs(save_model_dir)

            # Storage model configuration information
            save_setting_para_dir = 'Data/setting_para/' + datafile_name + '/' + model + '/' + date_str + '/'
            if os.path.exists(save_setting_para_dir):
                shutil.rmtree(save_setting_para_dir)
            os.makedirs(save_setting_para_dir)
            shutil.copy(data_setting_file, save_setting_para_dir)
            shutil.copy(model_setting_file, save_setting_para_dir)

            summary_step = 20
            early_stop_loss = loss
            val_early_stop_loss_min = 1000
            val_loss_noimprove_max_count = int(epochs / summary_step / 10)
            val_loss_no_improve_count = 0
            val_loss_best_step = 0

            run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
            run_metadata = tf.RunMetadata()

            batch_size = int(train_set.num_examples / 10)
            batch_size = 2 ** math.floor(math.log2(batch_size))

            with tf.Session() as sess:
                sess.run(init)
                # circuit training
                for step in range(epochs):
                    batch = train_set.next_batch(batch_size)
                    feed = {x_data: batch[0], y_data: batch[1], snn_x_data: batch[2], tnn_x_data: batch[3],
                            keep_prob_st: keep_prob_ratio,
                            keep_prob_gtw: keep_prob_ratio, bn_is_training: True}

                    #Generate run_metadata
                    if step % int(epochs / 5) == 0:
                        sess.run(train, feed_dict=feed, options=run_options, run_metadata=run_metadata)
                        writer_train.add_run_metadata(run_metadata, 'step%d' % step)
                    else:
                        sess.run(train, feed_dict=feed)

                    if step % summary_step == 0:
                        # training summary
                        current_epoch = int(step / 10)
                        [summary, train_loss, train_r2_p, train_r2_c, train_r2_adjusted_c, yhat_train_temp,
                         cur_learning_rate] = sess.run(
                            [summary_merged, loss, r2_pearson, r2_coeff, r2_adjusted_coeff, yhat, learning_rate_decay],
                            feed_dict=feed_train)
                        writer_train.add_summary(summary, current_epoch)

                        # validation summary
                        [summary, val_loss, val_loss_add_reg, val_early_stop_loss, val_r2_p, val_r2_c,
                         yhat_val_temp] = sess.run(
                            [summary_merged, loss, loss_add_reg, early_stop_loss, r2_pearson, r2_coeff, yhat],
                            feed_dict=feed_val)
                        writer_validation.add_summary(summary, current_epoch)

                        # test summary
                        [summary] = sess.run(
                            [summary_merged], feed_dict=feed_test)
                        writer_test.add_summary(summary, current_epoch)

                        if val_early_stopping:
                            if step >= val_early_stopping_begin_step:
                                if val_early_stop_loss - val_early_stop_loss_min < 0:
                                    val_early_stop_loss_min = val_early_stop_loss
                                    model_file_name = os.path.join(save_model_dir, 'model')
                                    try: 
                                        saver.save(sess,model_file_name, global_step=step)
                                    except Exception as e:
                                        print("Warning: Windows locked the checkpoint file. Skipped save to avoid crash.")
                                        print(e)   
                                    val_loss_no_improve_count = 0
                                    val_loss_best_step = step
                                else:
                                    val_loss_no_improve_count = val_loss_no_improve_count + 1
                                    if val_loss_no_improve_count >= val_loss_noimprove_max_count:
                                        break

                        print('Training_' + str(step) + ' loss_rg: ' + str(train_loss) + ', r2_pearson: ' + str(
                            train_r2_p) + ', r2_coeff: ' + str(train_r2_c) + ', r2_adjusted_coeff: ' + str(
                            train_r2_adjusted_c) + ', cur_learning_rate: ' + str(cur_learning_rate) + '.')
                        print('Valdation_' + str(step) + ' loss_rg: ' + str(val_loss) + ', r2_pearson: ' + str(
                            val_r2_p) + ', r2_coeff: ' + str(val_r2_c) + ', early_stop_loss: ' + str(
                            val_early_stop_loss) + ', val_loss_best_step: ' + str(val_loss_best_step) + '.')

                saver.restore(sess, saver.last_checkpoints[len(saver.last_checkpoints) - 1])
                print('Cross Validation ' + str(cv_index + 1) + '-----------Restore the best model!-----------')
                print('Early Stopping Best Step: ' + str(val_loss_best_step))

                # output
                output_indictors = [diagnosis.RSS, diagnosis.MS_train, diagnosis.MS_common, r2_coeff, r2_adjusted_coeff,
                                    diagnosis.A_R2, AICc, diagnosis.RMSE, ave_abs_error, ave_rel_error,
                                    diagnosis.r_pearson]
                output_indictor_names = ['RSS', 'MS_degree', 'MS_common', 'R2', 'Adjusted_R2_degree', 'Adjusted_R2',
                                         'AICc',
                                         'RMSE', 'MAE', 'MAPE', 'r']

                # training final result
                [train_loss, yhat_train, train_R2_pearson, train_R2_coeff, gtweight_train, gtbeta_train,
                 train_ave_abs_error, train_ave_rel_error, train_f1, train_f2, train_linear_beta,
                 train_f3_dict] = sess.run(
                    [loss, yhat, r2_pearson, r2_coeff, gtw_network.gtweight, gtbeta, ave_abs_error, ave_rel_error, f1,
                     f2,
                     linear_beta, f3_dict], feed_dict=feed_train)
                print('Training dataset loss_rg: ' + str(train_loss) + ', r2_pearson: ' + str(
                    train_R2_pearson) + ', r2_coeff: ' + str(train_R2_coeff) + '.')

                # validation final result
                [val_loss, yhat_val, val_R2_pearson, val_R2_coeff, gtweight_val, gtbeta_val, val_ave_abs_error,
                 val_ave_rel_error] = sess.run(
                    [loss, yhat, r2_pearson, r2_coeff, gtw_network.gtweight, gtbeta, ave_abs_error, ave_rel_error],
                    feed_dict=feed_val)
                print('Validation dataset loss_rg: ' + str(val_loss) + ', r2_pearson: ' + str(
                    val_R2_pearson) + ', r2_coeff: ' + str(
                    val_R2_coeff) + '.')

                # test result
                [test_loss, yhat_test, test_R2_pearson, test_R2_coeff, gtweight_test, gtbeta_test, test_ave_abs_error,
                 test_ave_rel_error] = sess.run(
                    [loss, yhat, r2_pearson, r2_coeff, gtw_network.gtweight, gtbeta, ave_abs_error, ave_rel_error],
                    feed_dict=feed_test)
                print('Testing dataset loss_rg: ' + str(test_loss) + ', r2_pearson: ' + str(
                    test_R2_pearson) + ', r2_coeff: ' + str(
                    test_R2_coeff) + '.')

                # Result output
                output_indictors_train = sess.run(output_indictors, feed_dict=feed_train)
                output_indictors_val = sess.run(output_indictors, feed_dict=feed_val)
                output_indictors_test = sess.run(output_indictors, feed_dict=feed_test)

                outputs_train.append(output_indictors_train)
                outputs_val.append(output_indictors_val)
                outputs_test.append(output_indictors_test)

                # Original data storage path after randomization
                file_save_path = dataset_path + dataname + '_' + str(cv_index + 1) + '.xls'
                # Result output path
                result_save_path = host_name + '/results/' + datafile_name + '/' + model + '/'
                if not os.path.exists(result_save_path):
                    os.makedirs(result_save_path)
                result_file = result_save_path + dataname + '_' + date_str + '.xls'

                rb = open_workbook(file_save_path)
                wb = copy(rb)

                wb = utils.add_result_excel(wb, rb, 'train', maxy, miny, yhat_train, gtbeta_train, gtweight_train,
                                            index=0, col_data_x=col_data_x, col_data_y=col_data_y, add_col=0)
                wb = utils.add_result_excel(wb, rb, 'validation', maxy, miny, yhat_val, gtbeta_val, gtweight_val,
                                            index=len(yhat_train), col_data_x=col_data_x, col_data_y=col_data_y,
                                            add_col=0)
                wb = utils.add_result_excel(wb, rb, 'test', maxy, miny, yhat_test, gtbeta_test, gtweight_test,
                                            index=len(yhat_train) + len(yhat_val), col_data_x=col_data_x,
                                            col_data_y=col_data_y, add_col=0)

                worksheet = wb.get_sheet('result')
                worksheet.write(0, 0, 'result')

                for i in range(len(output_indictor_names)):
                    worksheet.write(0, 1 + i, output_indictor_names[i])
                    worksheet.write(1, 1 + i, str(output_indictors_train[i]))
                    worksheet.write(2, 1 + i, str(output_indictors_val[i]))
                    worksheet.write(3, 1 + i, str(output_indictors_test[i]))

                curindex = len(output_indictor_names) + 1

                worksheet.write(0, curindex, 'f1')
                worksheet.write(1, curindex, train_f1.item())
                curindex = curindex + 1
                worksheet.write(0, curindex, 'f2')
                worksheet.write(1, curindex, train_f2.item())
                curindex = curindex + 1
                for i in range(len(train_f3_dict)):
                    worksheet.write(0, curindex, 'f3_param_' + str(i))
                    worksheet.write(1, curindex, train_f3_dict['f3_param_' + str(i)].item())
                    curindex = curindex + 1

                worksheet.write(1, 0, 'train')
                worksheet.write(2, 0, 'val')
                worksheet.write(3, 0, 'test')

                for i in range(len(col_data_x)):
                    worksheet.write(5, i, 'linear_' + col_data_x[i] + '_weight')
                    worksheet.write(6, i, train_linear_beta[i].item())

                worksheet.write(5, len(col_data_x), 'linear_constant_weight')
                worksheet.write(6, len(col_data_x), train_linear_beta[len(col_data_x)].item())

                if os.path.exists(result_file):
                    os.remove(result_file)
                wb.save(result_file)

        outputs_train = np.array(outputs_train)
        outputs_val = np.array(outputs_val)
        outputs_test = np.array(outputs_test)

        cv_train_result = outputs_train.mean(axis=0)
        cv_val_result = outputs_val.mean(axis=0)
        cv_test_result = outputs_test.mean(axis=0)

        print(datetime.datetime.now().strftime('%Y/%m/%d %H:%M:%S') + ' Cross Validation Result:')
        print('Training dataset loss: ' + str(cv_train_result[0]) + ', r2_coeff: ' + str(cv_train_result[3]) + '.')
        print('Validation dataset loss: ' + str(cv_val_result[0]) + ', r2_coeff: ' + str(cv_val_result[3]) + '.')
        print('Testing dataset loss: ' + str(cv_test_result[0]) + ', r2_coeff: ' + str(cv_test_result[3]) + '.')

        train_R2_coeff = cv_test_result[3]
        test_R2_coeff = cv_test_result[3]

        xls = xlwt.Workbook()
        worksheet = xls.add_sheet('result')
        worksheet.write(0, 0, 'result')
        for i in range(len(output_indictor_names)):
            worksheet.write(0, 1 + i, output_indictor_names[i])

        for cv_index in range(len(outputs_train)):
            worksheet.write(1 + cv_index * 3, 0, 'train_' + str(cv_index + 1))
            worksheet.write(2 + cv_index * 3, 0, 'val_' + str(cv_index + 1))
            worksheet.write(3 + cv_index * 3, 0, 'test_' + str(cv_index + 1))

            for i in range(len(output_indictor_names)):
                worksheet.write(1 + cv_index * 3, 1 + i, str(outputs_train[cv_index][i]))
                worksheet.write(2 + cv_index * 3, 1 + i, str(outputs_val[cv_index][i]))
                worksheet.write(3 + cv_index * 3, 1 + i, str(outputs_test[cv_index][i]))

        xls.save(host_name + '/results/' + datafile_name + '/' + model + '/' + dataname + '_' + date_str + '_cv.xls')
